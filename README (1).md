# ğŸ•¸ï¸ AI Web Scraper

AI Web Scraper is an intelligent web scraping application that extracts website content and uses **LLMs** to parse and analyze the scraped data.  
It is built with **Streamlit**, **Selenium**, **BeautifulSoup**, and **LangChain with Ollama** for natural language parsing.

---

## ğŸš€ Features
- ğŸŒ **Scrape Any Website** â€“ Extracts page content using Selenium + BeautifulSoup.  
- ğŸ§¹ **Cleaned DOM Content** â€“ Removes unnecessary scripts, styles, and formats the text.  
- âœ‚ï¸ **Chunking** â€“ Splits large DOM content into manageable chunks for processing.  
- ğŸ¤– **AI-Powered Parsing** â€“ Uses **LangChain + Ollama LLM** to parse and extract information based on user queries.  
- ğŸ“Š **Interactive UI** â€“ Streamlit-based web interface for scraping and querying data.  

---

## ğŸ› ï¸ Tech Stack
- [Python](https://www.python.org/)  
- [Streamlit](https://streamlit.io/) â€“ Web UI  
- [Selenium](https://www.selenium.dev/) â€“ Automated web scraping  
- [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) â€“ HTML parsing  
- [LangChain](https://www.langchain.com/) â€“ Orchestration framework  
- [Ollama](https://ollama.com/) â€“ LLM for natural language processing  

---

## ğŸ“‚ Project Structure
```
AI-Web-Scraper/
â”‚â”€â”€ main.py              # Streamlit UI entry point
â”‚â”€â”€ scrape.py            # Scraping utilities (Selenium + BeautifulSoup)
â”‚â”€â”€ parse.py             # Parsing logic with LangChain + Ollama
â”‚â”€â”€ requirements.txt     # Project dependencies
â”‚â”€â”€ chromedriver.exe     # Chrome WebDriver (for Selenium)
```

---

## âš™ï¸ Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/your-username/ai-web-scraper.git
   cd ai-web-scraper
   ```

2. **Set up a virtual environment (recommended)**
   ```bash
   python -m venv venv
   source venv/bin/activate   # On Linux/Mac
   venv\Scripts\activate      # On Windows
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set environment variables** (optional)
   - Create a `.env` file to configure custom settings like `TARGET_URL`.

---

## â–¶ï¸ Usage

Run the Streamlit app:
```bash
streamlit run main.py
```

- Enter a **website URL** in the text box.  
- Scrape and view the **cleaned DOM content**.  
- Describe what you want to extract (e.g., "List all product names and prices").  
- Let the AI parse the results using **Ollama LLM**.  

---

## ğŸ“ Example Workflow
1. Input: `https://example.com/products`  
2. Query: `"Extract all product names and their prices"`  
3. Output: AI returns a structured list of products with corresponding prices.  

---

## ğŸ”® Future Improvements
- [ ] Add database storage (SQLite / MongoDB)  
- [ ] Enable export to CSV/Excel  
- [ ] Add support for multiple LLM providers  
- [ ] Enhance CAPTCHA handling  

---

## ğŸ“œ License
This project is licensed under the **MIT License**. Feel free to use and modify it.
